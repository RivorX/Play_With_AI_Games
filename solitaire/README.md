# Solitaire AI (Klondike)

Ten projekt zawiera Å›rodowisko Pasjansa (Klondike) oraz skrypty do trenowania agenta AI przy uÅ¼yciu algorytmu **MaskablePPO** (Stable Baselines 3).

ğŸš€ **Projekt zostaÅ‚ ulepszon na podstawie zaawansowanej architektury Snake AI** - zawiera attention mechanism, dropout regularization, mixed precision training i zaawansowane callbacki.

---

## PrzykÅ‚adowy przebieg (GIF) â€” szybki podglÄ…d

![Solitaire Run GIF](./docs/solitaire_run.gif)

---

## Struktura

- `config/config.yaml`: Konfiguracja Å›rodowiska i modelu.
- `scripts/model.py`: Implementacja Å›rodowiska Solitaire (Gymnasium).
- `scripts/cnn.py`: Ekstraktor cech (Feature Extractor) z attention mechanism.
- `scripts/train.py`: Skrypt treningowy z zaawansowanymi callbackami.
- `scripts/test_solitaire_model.py`: Wizualizacja rozgrywki agenta w oknie Pygame.
- `scripts/debug/make_gif.py`: Nagrywanie rozgrywki do pliku GIF.
- `scripts/utils/`: Utility functions (callbacks, plotting).
- `models/`: Katalog na zapisane modele.
- `logs/`: Logi treningowe i wykresy postÄ™pu.

---

## Uruchomienie treningu

Aby rozpoczÄ…Ä‡ trening, uruchom skrypt `train.py` z katalogu gÅ‚Ã³wnego:

```bash
python ./solitaire/scripts/train.py
```

**Nowe funkcje treningu:**
- âœ… **32 rÃ³wnolegÅ‚e Å›rodowiska** (SubprocVecEnv)
- âœ… **AdamW optimizer** z weight decay
- âœ… **Entropy scheduler** - automatyczne zmniejszanie entropii
- âœ… **Win tracker** - Å›ledzenie wspÃ³Å‚czynnika wygranych
- âœ… **Auto-stop** przy braku postÄ™pÃ³w
- âœ… **Auto-plotting** po kaÅ¼dej ewaluacji

---

## Testowanie modelu

### Wizualizacja w oknie Pygame
Aby zobaczyÄ‡ grÄ™ agenta w czasie rzeczywistym:

```bash
python ./solitaire/scripts/test_solitaire_model.py
```

FunkcjonalnoÅ›ci:
- Interaktywny wybÃ³r modelu (best_model.zip lub solitaire_ppo_model.zip)
- Graficzna wizualizacja stanu gry
- WyÅ›wietlanie podjÄ™tych akcji, nagrÃ³d i wynikÃ³w
- Komunikaty o przegranej/wygranej/poddaniu siÄ™

### Nagrywanie do GIF-a
Aby nagraÄ‡ rozgrywkÄ™ w formacie GIF:

```bash
python ./solitaire/scripts/debug/make_gif.py --episodes 1 --fps 2
```

Parametry:
- `--episodes`: Liczba epizodÃ³w do nagrania (default: 1)
- `--fps`: Klatki na sekundÄ™ (default: 2)
- `--pause`: Czas pauzy na ostatniej klatce w sekundach (default: 3.0)
- `--out`: Niestandardowa Å›cieÅ¼ka wyjÅ›ciowa GIF-a

Wynik zapisywany jest do `solitaire/logs/solitaire_run.gif`.

## Zasady (Uproszczone)

- **Draw Stock**: Dobieranie karty z talii.
- **Ruchy**:
  - Z talii (Waste) na stÃ³Å‚ (Tableau).
  - Z talii (Waste) na stosy bazowe (Foundations).
  - MiÄ™dzy kolumnami na stole (Tableau -> Tableau).
  - Ze stoÅ‚u na stosy bazowe (Tableau -> Foundations).
  - Z talii (Stock) na stosy bazowe (Stock -> Foundations).
  - Poddanie siÄ™ (Surrender) - agent moÅ¼e zakoÅ„czyÄ‡ grÄ™, gdy uzna jÄ… za beznadziejnÄ….
- **Cel**: UmieÅ›ciÄ‡ wszystkie karty na stosach bazowych (Foundations).

## Architektura Sieci Neuronowej ğŸ§ 

### Feature Extractor (cnn.py) - Ulepszona Architektura

SieÄ‡ wykorzystuje zaawansowanÄ… architekturÄ™ z komponentami zainspirowanymi Snake AI:

#### Komponenty sieci:

1. **Tableau Network** (7Ã—20Ã—4 â†’ 256 cech)
   - GÅ‚Ä™boka sieÄ‡: [384, 256] z dropout (0.02)
   - LayerNorm + GELU activation
   - **Attention mechanism** - fokus na waÅ¼nych kartach
   
2. **Foundations Network** (4 â†’ 64 cechy)
   - Pojedyncza warstwa z LayerNorm
   - Brak dropout (maÅ‚e wejÅ›cie)

3. **Waste Network** (3 â†’ 64 cechy)
   - Pojedyncza warstwa z LayerNorm
   - Brak dropout (maÅ‚e wejÅ›cie)

4. **Stock Network** (52Ã—3 â†’ 96 cech)
   - GÅ‚Ä™boka sieÄ‡: [128, 96] z dropout (0.02)
   - LayerNorm + GELU activation
   - **Attention mechanism** - fokus na dostÄ™pnych kartach

5. **Fusion Network** (480 â†’ 512 cech)
   - ÅÄ…czy wszystkie komponenty
   - LayerNorm + GELU + Dropout (0.04)

#### Zaawansowane funkcje:

- âœ… **Attention Mechanism** - automatyczne skupienie na waÅ¼nych elementach stanu
- âœ… **BF16 Mixed Precision** - szybszy trening na GPU (~30% przyspieszenie)
- âœ… **LayerNorm** - stabilizacja treningu
- âœ… **GELU Activation** - lepsza od ReLU w deep learning
- âœ… **Dropout Regularization** - zapobiega overfittingowi
- âœ… **He Initialization** - poprawna inicjalizacja wag

#### Policy & Value Networks:

- **Policy (Actor)**: [512, 256, 128] â†’ 87 akcji
- **Value (Critic)**: [512, 512, 256] â†’ 1 wartoÅ›Ä‡ stanu

**CaÅ‚kowita liczba parametrÃ³w**: ~800K-1.2M (zaleÅ¼nie od konfiguracji)

---

## PostÄ™p Treningu ğŸ“Š

PoniÅ¼sze wykresy pokazujÄ… postÄ™p treningu modelu (8 metryk w ukÅ‚adzie 4Ã—2):

### Training Progress
![Training Progress](./docs/training_progress.png)

**Nowe metryki (dodane w ulepszeniu):**

1. **Mean Reward** - Åšrednia nagroda (wygÅ‚adzona)
   - Ewolucja Å›redniej nagrody na ostatnie 100 epizodÃ³w
   - Agent systematycznie uczy siÄ™ graÄ‡ lepiej

2. **Win Rate** - WspÃ³Å‚czynnik wygranych
   - Zaczyna od ~0% i roÅ›nie do ~15-20%
   - Pasjans Klondike jest bardzo trudny (nawet czÅ‚owiek wygrywa w ~15-20%)
   - Agent osiÄ…ga wyniki porÃ³wnywalne z czÅ‚owiekiem

3. **Mean Score** - Åšredni wynik w grze
   - Suma nagrÃ³d z poszczegÃ³lnych akcji
   - WyÅ¼szy score = lepsza strategia

4. **Max Score** - Maksymalny wynik osiÄ…gniÄ™ty
   - Tracking najlepszych gier
   - Pokazuje potencjaÅ‚ agenta

5. **Mean Foundations Filled** - Åšrednia liczba kart na fundacjach
   - 0-52 karty (cel: 52)
   - Im wyÅ¼ej, tym bliÅ¼ej wygranej

6. **Mean Episode Length** - Åšrednia dÅ‚ugoÅ›Ä‡ epizodu
   - DÅ‚uÅ¼sze epizody = agent nie poddaje siÄ™ szybko
   - Wzrost z ~10 do ~100-150 krokÃ³w pokazuje lepszÄ… strategiÄ™

7. **Mean Moves per Game** - Åšrednia liczba ruchÃ³w
   - EfektywnoÅ›Ä‡ gry
   - Mniej ruchÃ³w = lepsza strategia

8. **Combined Performance** - Znormalizowane metryki
   - Win Rate, Score, Foundations na jednym wykresie
   - Åatwe porÃ³wnanie postÄ™pÃ³w

**Monitoring w czasie rzeczywistym:**
- CSV: `logs/train_progress.csv` (11 kolumn)
- TensorBoard: `logs/ppo_X/`
- Wykresy: Auto-update po kaÅ¼dej ewaluacji

---

## Konfiguracja âš™ï¸

Wszystkie parametry sÄ… zdefiniowane w `config.yaml`:

### Nagrody (Reward Scaling)
- `move_to_foundation`: +20 (gÅ‚Ã³wny cel - karta na fundacjÄ™)
- `flip_tableau_card`: +7 (odkrywanie kart)
- `move_waste_to_tableau`: +2 (przygotowanie)
- `move_tableau_to_tableau`: -0.5 (zniechÄ™canie do zbyt wielu przesuniÄ™Ä‡)
- `win_bonus`: +2000 (wygrana gry!)
- `invalid_move_penalty`: -0.5 (kara za niewalidny ruch)
- `time_penalty`: -0.1 (kara za kaÅ¼dy krok)
- `recycle_waste_penalty`: -10 (kara za przetasowanie talii)
- `surrender_penalty`: -100 (kara za poddanie siÄ™)

### Parametry Modelu (MaskablePPO)
- `learning_rate`: 0.0003 (poczÄ…tkowy)
- `min_learning_rate`: 0.00001
- `n_steps`: 4096 (kroki na aktualizacjÄ™)
- `batch_size`: 4096
- `n_epochs`: 8
- `gamma`: 0.995 (dyskont przyszÅ‚ych nagrÃ³d)
- `gae_lambda`: 0.97
- `clip_range`: 0.2
- `ent_coef`: 0.03 â†’ 0.005 (scheduler)
- `vf_coef`: 0.5

### Optimizer (AdamW)
- `type`: adamw
- `weight_decay`: 0.0001 (regularizacja L2)
- `eps`: 1e-8
- `betas`: [0.9, 0.999]

### Dropout (Regularization)
- `tableau_dropout`: 0.02
- `stock_dropout`: 0.02
- `fusion_dropout`: 0.04
- `foundations_dropout`: 0.0 (maÅ‚e wejÅ›cie)
- `waste_dropout`: 0.0 (maÅ‚e wejÅ›cie)

### Parametry Treningu
- `n_envs`: 32 (rÃ³wnolegÅ‚e Å›rodowiska)
- `total_timesteps`: 20,000,000
- `eval_freq`: 16,384
- `eval_n_envs`: 4
- `eval_n_repeats`: 3
- `max_no_improvement_evals`: 50 (auto-stop)
- `min_evals`: 20

---

## PrzykÅ‚ad Rozgrywki ğŸ®

GIF pokazany na poczÄ…tku dokumentacji zawiera przykÅ‚adowÄ… rozgrywkÄ™ agenta w akcji:
- Agent wybiera ruchy w oparciu o wytrenowany model neuronowy
- **Attention mechanism** pomaga skupiÄ‡ siÄ™ na najwaÅ¼niejszych kartach
- Karty z talii sÄ… stopniowo odkrywane
- Karty sÄ… przenoszone na stosy bazowe (Foundations) gdy jest to moÅ¼liwe
- Agent wykorzystuje strategiÄ™, aby maksymalizowaÄ‡ szanse na zwyciÄ™stwo

---

## Normalizacja Danych

Wszystkie dane wejÅ›ciowe sÄ… znormalizowane do zakresu [0.0, 1.0]:
- Ranga karty: `rank / 13.0`
- Kolor karty: `suit / 3.0`
- ObecnoÅ›Ä‡ karty: `0.0` lub `1.0`
- Face up/down: `0.0` lub `1.0`

To znacznie poprawia stabilnoÅ›Ä‡ treningu sieci neuronowej.

---

## Wymagania

```
stable-baselines3
sb3-contrib
torch
gymnasium
pygame
matplotlib
numpy
pyyaml
```

---

## Licencja

MIT License - projekt edukacyjny

---

**Powodzenia w treningu! ğŸ²ğŸƒ**
