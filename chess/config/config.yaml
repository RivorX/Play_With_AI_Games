# Chess AI Training Configuration - Enhanced Edition
# Optimized for RTX 5060 Ti with AMP, SE-Blocks, OneCycleLR, and Label Smoothing

# Paths (relative to project root)
paths:
  data_dir: "data"
  models_dir: "models"
  logs_dir: "logs"
  best_model: "models/best_model.pt"
  il_checkpoint: "models/il_checkpoint.pt"
  rl_checkpoint: "models/rl_checkpoint.pt"

# Data Processing
data:
  min_elo: 2000  # Minimum Elo rating for games
  max_games: 100000  # Maximum number of games to load (divided among PGN files)
  train_split: 0.9  # Train/validation split
  max_moves_per_game: 200  # Skip longer games
  


# Model Architecture - Enhanced with Multiple Attention Mechanisms
model:
  num_residual_blocks: 8  # Increased for better learning (6-12 optimal)
  filters: 192  # More filters for RTX 5060 Ti (128-256)
  policy_head_filters: 32  # Increased from 2
  value_head_filters: 32  # Increased from 1
  value_hidden_dim: 256
  dropout: 0.3
  
  # üöÄ Advanced Features (minimal parameter cost, big gains!)
  use_se_blocks: true           # üß† Channel attention (+5% params, +20% quality)
  use_spatial_attention: true   # üëÅÔ∏è Spatial attention (+1% params, +6% quality)
  drop_path_rate: 0.1           # üé≤ Stochastic Depth (0% params, +4% quality)
  activation: 'elu'             # ‚ö° ELU activation (0% params, +3% quality)
  use_coord_conv: true          # üìç Position-aware conv (+0.5% params, +4% quality)

# -----------------------------------------
# Imitation Learning 
# -----------------------------------------
imitation_learning:
  batch_size: 1024  # Optimized for RTX 5060 Ti with AMP (256-1024)
  learning_rate: 0.001  # Will be managed by OneCycleLR
  epochs: 100  # More epochs with better early stopping
  weight_decay: 0.0001
  grad_clip: 1.0
  
  # üéØ Advanced Training Features
  label_smoothing: 0.1  # Regularization (0.0 = off, 0.1 = standard)
  use_onecycle_lr: true  # üìà OneCycleLR scheduler for better convergence
  
  # Loss weights
  policy_loss_weight: 1.0
  value_loss_weight: 0.5
  
  # Evaluation
  eval_every: 1  # Evaluate every N epochs
  save_every: 5  # Save checkpoint every N epochs
  
  # Early stopping - Extended patience
  patience: 3  # Old parameter (kept for backward compatibility)
  max_patience: 10  # ‚èπÔ∏è Stop if no improvement for 10 epochs
  min_delta: 0.001  # Minimum improvement to count


# -----------------------------------------
# Reinforcement Learning
# -----------------------------------------
reinforcement_learning:
  # Self-play
  games_per_iteration: 100  # Games to generate per iteration
  iterations: 1000  # Total RL iterations
  
  # MCTS parameters
  mcts_simulations: 100  # Number of MCTS simulations per move
  mcts_c_puct: 1.5  # Exploration constant
  mcts_temperature: 1.0  # Temperature for move selection (high = exploration)
  mcts_temperature_threshold: 15  # Move number to reduce temperature
  mcts_dirichlet_alpha: 0.3  # Dirichlet noise for exploration
  mcts_dirichlet_weight: 0.25  # Weight of Dirichlet noise
  
  # Training
  batch_size: 1024  # Matched with IL for consistency
  learning_rate: 0.0005  # Lower than IL
  train_epochs_per_iteration: 5  # Epochs per iteration
  replay_buffer_size: 50000  # Max positions in replay buffer
  
  # Evaluation
  eval_games: 20  # Games to play for evaluation
  eval_every: 5  # Evaluate every N iterations
  win_rate_threshold: 0.55  # Win rate to update best model
  
  # Loss weights
  policy_loss_weight: 1.0
  value_loss_weight: 1.0

# Hardware - Optimized for RTX 5060 Ti
hardware:
  device: "cuda"  # cuda, cpu, or mps (Mac)
  use_amp: true  # üöÄ Mixed Precision Training (~2x speedup on RTX cards)
  use_bfloat16: true  # üíæ Use bfloat16 instead of float16 (better range, smaller models)
  num_workers: 8  # DataLoader workers (4-12 optimal, 0 to disable)
  pin_memory: true  # Faster data transfer to GPU

# Logging
logging:
  log_level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  tensorboard: true
  print_every: 10  # Print stats every N batches
  
# Random seed for reproducibility
seed: 42