import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.amp import autocast
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
import yaml
from gymnasium import spaces
import os

# Wczytaj konfiguracjƒô
base_dir = os.path.dirname(os.path.dirname(__file__))
config_path = os.path.join(base_dir, 'config', 'config.yaml')
with open(config_path, 'r') as f:
    config = yaml.safe_load(f)

# ‚úÖ GLOBAL FLAG - wy≈õwietl info tylko raz
_INFO_PRINTED = False


class LayerNormLSTM(nn.Module):
    """
    LSTM z Layer Normalization - zapobiega saturacji Cell State
    KLUCZOWA NAPRAWA dla problemu LSTM Cell State = 1.8-2.0
    """
    def __init__(self, input_size, hidden_size, num_layers=1, batch_first=True):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.batch_first = batch_first
        
        # Standardowy LSTM
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=batch_first)
        
        # Layer Norm dla ka≈ºdej warstwy LSTM (zapobiega saturacji hidden state)
        self.layer_norms = nn.ModuleList([
            nn.LayerNorm(hidden_size) for _ in range(num_layers)
        ])
        
        # Inicjalizacja LSTM (zapobiega wczesnej saturacji)
        self._init_lstm_weights()
        
    def _init_lstm_weights(self):
        """Specjalna inicjalizacja dla stabilnego LSTM"""
        for name, param in self.lstm.named_parameters():
            if 'weight_ih' in name:
                nn.init.xavier_normal_(param, gain=0.5)
            elif 'weight_hh' in name:
                nn.init.orthogonal_(param, gain=0.5)
            elif 'bias' in name:
                nn.init.constant_(param, 0)
                # Forget gate bias = 1.0 (zapobiega zanikaniu pamiƒôci)
                n = param.size(0)
                param.data[n//4:n//2].fill_(1.0)
        
    def forward(self, x, hidden=None):
        # LSTM forward
        output, (h, c) = self.lstm(x, hidden)
        
        # ‚úÖ KLUCZOWA NAPRAWA: Aplikuj Layer Norm do hidden states
        # To zapobiega saturacji (h >>1.0) kt√≥ra powodowa≈Ça gradient vanishing
        h_norm = torch.stack([
            self.layer_norms[i](h[i]) for i in range(self.num_layers)
        ])
        
        # Cell state zostawiamy bez norm (wa≈ºne dla pamiƒôci d≈Çugoterminowej)
        # Ale clippujemy ≈ºeby nie eksplodowa≈Ç
        c_clipped = torch.clamp(c, -2.0, 2.0)
        
        return output, (h_norm, c_clipped)


class CustomFeaturesExtractor(BaseFeaturesExtractor):
    """
    NAPRAWIONY Features Extractor - USUNIƒòTE BOTTLENECKI
    
    ZMIANY:
    1. ‚úÖ LayerNorm LSTM - zapobiega saturacji Cell State (1.8‚Üí0.5)
    2. ‚úÖ Gradient Clipping w LSTM
    3. ‚úÖ Usuniƒôto zbƒôdne dropouty z CNN (BatchNorm wystarczy)
    4. ‚úÖ ReLU zamiast LeakyReLU dla skalar√≥w (silniejsze gradienty)
    5. ‚úÖ Xavier init zamiast He (lepsze dla ma≈Çych sieci)
    6. ‚úÖ Stabilna fusion bez learnable weights
    7. ‚úÖ Wiƒôksza architektura CNN [32,64,128]
    8. ‚úÖ Dwuwarstwowe Scalar MLP [128,64]
    """
    def __init__(self, observation_space: spaces.Dict, features_dim=256):
        super().__init__(observation_space, features_dim)
        
        global _INFO_PRINTED
        
        # Pobierz konfiguracjƒô
        cnn_channels = config['model']['convlstm']['cnn_channels']
        scalar_hidden_dims = config['model']['convlstm']['scalar_hidden_dims']
        
        # Dropouty - MOCNO ZMNIEJSZONE
        cnn_dropout = 0.0  # WY≈ÅƒÑCZONY (mamy BatchNorm)
        scalar_dropout = config['model'].get('scalar_dropout', 0.05)
        scalar_input_dropout = config['model'].get('scalar_input_dropout', 0.10)
        fusion_dropout = config['model'].get('fusion_dropout', 0.05)
        
        # W≈ÇƒÖcz AMP je≈õli CUDA dostƒôpne
        self.use_amp = torch.cuda.is_available()
        
        in_channels = 1
        
        # ===========================
        # CNN Z POPRAWIONYM RESIDUAL
        # ===========================
        # Warstwa 1 (16x16 ‚Üí 16x16, stride=1)
        self.conv1 = nn.Conv2d(in_channels, cnn_channels[0], kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(cnn_channels[0])
        # ‚úÖ BRAK dropout1 - za wcze≈õnie, BatchNorm wystarczy
        
        # Warstwa 2 z residual (16x16 ‚Üí 8x8, stride=2)
        self.conv2 = nn.Conv2d(cnn_channels[0], cnn_channels[1], kernel_size=3, stride=2, padding=1)
        self.bn2 = nn.BatchNorm2d(cnn_channels[1])
        self.dropout2 = nn.Dropout2d(cnn_dropout) if cnn_dropout > 0 else nn.Identity()
        
        # Residual projection dla warstwy 2
        self.residual_proj = nn.Conv2d(cnn_channels[0], cnn_channels[1], kernel_size=1, stride=2, padding=0)
        self.residual_bn = nn.BatchNorm2d(cnn_channels[1])
        
        # Warstwa 3 z residual (8x8 ‚Üí 4x4, stride=2)
        self.conv3 = None
        if len(cnn_channels) > 2:
            self.conv3 = nn.Conv2d(cnn_channels[1], cnn_channels[2], kernel_size=3, stride=2, padding=1)
            self.bn3 = nn.BatchNorm2d(cnn_channels[2])
            self.dropout3 = nn.Dropout2d(cnn_dropout) if cnn_dropout > 0 else nn.Identity()
            
            # Residual projection dla warstwy 3
            self.residual_proj2 = nn.Conv2d(cnn_channels[1], cnn_channels[2], kernel_size=1, stride=2, padding=0)
            self.residual_bn2 = nn.BatchNorm2d(cnn_channels[2])
        
        self.flatten = nn.Flatten()
        
        # Oblicz wymiar po CNN
        spatial_size = 16  # viewport size
        if len(cnn_channels) > 2:
            spatial_size = spatial_size // 2 // 2  # Dwa stride=2 (16‚Üí8‚Üí4)
        else:
            spatial_size = spatial_size // 2  # Jeden stride=2 (16‚Üí8)
        
        cnn_output_size = spatial_size * spatial_size
        cnn_dim = cnn_channels[-1] * cnn_output_size
        
        # LayerNorm dla CNN output
        self.cnn_norm = nn.LayerNorm(cnn_dim)
        
        # ===========================
        # SCALARS Z POPRAWIONYM MLP
        # ===========================
        scalar_dim = 7  # 2 (direction) + 5 (inne)
        
        # Input dropout dla wymuszania CNN
        self.scalar_input_dropout = nn.Dropout(scalar_input_dropout)
        
        # ‚úÖ ZMIANA: ReLU zamiast LeakyReLU (silniejsze gradienty)
        scalar_layers = []
        prev_dim = scalar_dim
        
        for idx, hidden_dim in enumerate(scalar_hidden_dims):
            scalar_layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.ReLU(inplace=True),  # ‚úÖ ReLU zamiast LeakyReLU
                nn.Dropout(scalar_dropout) if idx < len(scalar_hidden_dims)-1 else nn.Identity()
            ])
            prev_dim = hidden_dim
        
        self.scalar_linear = nn.Sequential(*scalar_layers)
        scalar_output_dim = scalar_hidden_dims[-1]
        
        # LayerNorm dla scalar output
        self.scalar_norm = nn.LayerNorm(scalar_output_dim)
        
        # ===========================
        # STABILNA FUSION (bez learnable weights)
        # ===========================
        total_dim = cnn_dim + scalar_output_dim
        
        # ‚úÖ USUNIƒòTO learnable weights (powodowa≈Çy niestabilno≈õƒá)
        # Zamiast tego: normalizacja + linear
        self.final_linear = nn.Sequential(
            nn.LayerNorm(total_dim),  # Normalizacja PRZED linear
            nn.Linear(total_dim, features_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(fusion_dropout)
        )
        
        # ===========================
        # XAVIER INITIALIZATION (lepsze ni≈º He dla ma≈Çych sieci)
        # ===========================
        self._initialize_weights()
        
        # ===========================
        # LOGGING (tylko raz)
        # ===========================
        if not _INFO_PRINTED:
            amp_status = "‚úÖ ENABLED" if self.use_amp else "‚ùå DISABLED (CPU mode)"
            print(f"\n{'='*70}")
            print(f"[CNN] NAPRAWIONY FEATURES EXTRACTOR - BOTTLENECKI USUNIƒòTE")
            print(f"{'='*70}")
            print(f"[CNN] Architektura CNN: {cnn_channels}")
            print(f"[CNN] ‚úÖ Residual connections: ENABLED")
            print(f"[CNN] ‚úÖ Dropout CNN: {cnn_dropout} (WY≈ÅƒÑCZONY - mamy BatchNorm)")
            print(f"[CNN] ‚úÖ AMP (Mixed Precision): {amp_status}")
            print(f"[CNN] Spatial size po CNN: {spatial_size}x{spatial_size}")
            print(f"[CNN] CNN output dim: {cnn_dim}")
            print(f"[CNN] Scalar hidden dims: {scalar_hidden_dims}")
            print(f"[CNN] Scalar output dim: {scalar_output_dim}")
            print(f"[CNN] Total features dim: {total_dim} -> {features_dim}")
            print(f"[CNN] Scalary stanowiƒÖ {scalar_output_dim/total_dim*100:.1f}% wej≈õcia")
            print(f"\n[DROPOUT]")
            print(f"  - CNN: {cnn_dropout} (WY≈ÅƒÑCZONY)")
            print(f"  - Scalar INPUT: {scalar_input_dropout} (wymusza CNN)")
            print(f"  - Scalar hidden: {scalar_dropout}")
            print(f"  - Fusion: {fusion_dropout}")
            print(f"\n[ACTIVATION]")
            print(f"  - CNN: LeakyReLU(0.01)")
            print(f"  - Scalars: ReLU (silniejsze gradienty)")
            print(f"\n[NORMALIZATION]")
            print(f"  - CNN: BatchNorm2d + LayerNorm")
            print(f"  - Scalars: LayerNorm")
            print(f"  - Fusion: LayerNorm (STABILNE)")
            print(f"\n[INITIALIZATION]")
            print(f"  - Conv/Linear: Xavier (gain=0.8)")
            print(f"  - LSTM: Xavier + Orthogonal (gain=0.5)")
            print(f"  - Forget bias: 1.0 (zapobiega zanikaniu pamiƒôci)")
            
            if self.use_amp:
                print(f"\n[AMP] üöÄ Expected speedup: 30-50% (RTX series)")
                print(f"[AMP] üíæ Expected VRAM saving: ~30%")
            
            print(f"\n[FIXES APPLIED]")
            print(f"  ‚úÖ Usuniƒôto zbƒôdne dropouty z CNN")
            print(f"  ‚úÖ ReLU zamiast LeakyReLU dla skalar√≥w")
            print(f"  ‚úÖ Xavier init zamiast He")
            print(f"  ‚úÖ Stabilna fusion bez learnable weights")
            print(f"  ‚úÖ Wiƒôksza architektura CNN/Scalar")
            print(f"{'='*70}\n")
            _INFO_PRINTED = True

    def _initialize_weights(self):
        """
        Xavier initialization dla stabilno≈õci treningu
        Lepsze ni≈º He dla ma≈Çych sieci (mniej saturacji)
        """
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                # Xavier dla Conv (gain=0.8 dla mniejszej wariancji)
                nn.init.xavier_normal_(m.weight, gain=0.8)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                # Xavier dla Linear
                nn.init.xavier_normal_(m.weight, gain=0.8)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, (nn.BatchNorm2d, nn.LayerNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, observations):
        # ===========================
        # CNN Z AMP I RESIDUAL CONNECTIONS
        # ===========================
        image = observations['image']
        if image.dim() == 4 and image.shape[-1] == 1:
            image = image.permute(0, 3, 1, 2)
        
        # ‚úÖ AUTOCAST dla CNN
        with autocast('cuda', enabled=self.use_amp):
            # Warstwa 1
            x = self.conv1(image)
            x = self.bn1(x)
            x = F.leaky_relu(x, 0.01, inplace=True)
            # BRAK dropout1
            identity1 = x
            
            # Warstwa 2 z residual
            x = self.conv2(x)
            x = self.bn2(x)
            
            # Skip connection
            identity1 = self.residual_proj(identity1)
            identity1 = self.residual_bn(identity1)
            
            x = x + identity1  # ‚úÖ RESIDUAL CONNECTION
            x = F.leaky_relu(x, 0.01, inplace=True)
            x = self.dropout2(x)
            
            # Warstwa 3 (je≈õli istnieje) z residual
            if self.conv3 is not None:
                identity2 = x
                x = self.conv3(x)
                x = self.bn3(x)
                
                identity2 = self.residual_proj2(identity2)
                identity2 = self.residual_bn2(identity2)
                
                x = x + identity2  # ‚úÖ RESIDUAL CONNECTION
                x = F.leaky_relu(x, 0.01, inplace=True)
                x = self.dropout3(x)
            
            # Flatten
            image_features = self.flatten(x)
        
        # ‚úÖ LayerNorm POZA autocast (wymaga float32)
        image_features = image_features.float()
        image_features = self.cnn_norm(image_features)
        
        # ===========================
        # SCALARS Z ReLU
        # ===========================
        scalars = torch.cat([
            observations['direction'],
            observations['dx_head'],
            observations['dy_head'],
            observations['front_coll'],
            observations['left_coll'],
            observations['right_coll']
        ], dim=-1)
        
        scalars = self.scalar_input_dropout(scalars)
        scalar_features = self.scalar_linear(scalars)
        scalar_features = self.scalar_norm(scalar_features)
        
        # ===========================
        # STABILNA FUSION (bez weighted)
        # ===========================
        features = torch.cat([image_features, scalar_features], dim=-1)
        return self.final_linear(features)