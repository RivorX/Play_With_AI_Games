import os
import sys
import yaml
import pickle
import torch
import gc
from sb3_contrib import RecurrentPPO
from stable_baselines3.common.vec_env import SubprocVecEnv, VecNormalize
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import StopTrainingOnNoModelImprovement
import matplotlib
matplotlib.use('Agg')

# Import lokalnych moduÅ‚Ã³w
from model import make_env
from cnn import CustomFeaturesExtractor
from utils.callbacks import TrainProgressCallback, CustomEvalCallback, LossRecorderCallback, EntropySchedulerCallback, VictoryTrackerCallback
from utils.gradient_monitor import GradientWeightMonitor
from utils.training_utils import (
    linear_schedule, 
    entropy_schedule,
    apply_gradient_clipping, 
    ensure_directories,
    reset_channel_logs,
    init_channel_loggers,
    log_observation
)

# Wczytaj konfiguracjÄ™
base_dir = os.path.dirname(os.path.dirname(__file__))
config_path = os.path.join(base_dir, 'config', 'config.yaml')
with open(config_path, 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)

ensure_directories(base_dir)

# Channel logging (opcjonalne)
enable_channel_logs = config.get('training', {}).get('enable_channel_logs', False)
channel_loggers = init_channel_loggers(base_dir) if enable_channel_logs else {}


def clear_gpu_cache():
    """ğŸ§¹ WyczyÅ›Ä‡ cache GPU i usuÅ„ nieuÅ¼ywane obiekty"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()


def setup_adamw_optimizer(model, config):
    """
    Konfiguruje optimizer AdamW dla modelu RecurrentPPO
    
    Args:
        model: Model RecurrentPPO
        config: Konfiguracja z config.yaml
    """
    opt_config = config['model'].get('optimizer', {})
    optimizer_type = opt_config.get('type', 'adam').lower()
    weight_decay = opt_config.get('weight_decay', 0.01)
    eps = opt_config.get('eps', 1e-5)
    betas = tuple(opt_config.get('betas', [0.9, 0.999]))
    
    if optimizer_type == 'adamw':
        # âœ… UtwÃ³rz AdamW optimizer
        optimizer = torch.optim.AdamW(
            model.policy.parameters(),
            lr=model.learning_rate if isinstance(model.learning_rate, float) else model.learning_rate(1.0),
            betas=betas,
            eps=eps,
            weight_decay=weight_decay
        )
        
        # ZastÄ…p optimizer w modelu
        model.policy.optimizer = optimizer
        
        print(f"\n{'='*70}")
        print(f"[OPTIMIZER] âœ… AdamW ENABLED")
        print(f"{'='*70}")
        print(f"  Type:          AdamW")
        print(f"  Weight Decay:  {weight_decay} {'(L2 regularization)' if weight_decay > 0 else '(DISABLED)'}")
        print(f"  Epsilon:       {eps}")
        print(f"  Betas:         {betas}")
        print(f"  Learning Rate: {model.learning_rate if isinstance(model.learning_rate, float) else 'schedule'}")
        print(f"{'='*70}\n")
        
    elif optimizer_type == 'adam':
        # DomyÅ›lny Adam (juÅ¼ jest w SB3)
        print(f"\n{'='*70}")
        print(f"[OPTIMIZER] Standard Adam (default SB3)")
        print(f"{'='*70}")
        print(f"  Type:          Adam")
        print(f"  Epsilon:       {eps}")
        print(f"  Betas:         {betas}")
        print(f"{'='*70}\n")
    else:
        raise ValueError(f"Unknown optimizer type: {optimizer_type}. Use 'adam' or 'adamw'.")
    
    return model


def train(use_progress_bar=False, use_config_hyperparams=True):
    global best_model_save_path

    n_envs = config['training']['n_envs']
    n_steps = config['model']['n_steps']
    eval_freq = config['training']['eval_freq']
    plot_interval = config['training']['plot_interval']
    eval_n_envs = config['training'].get('eval_n_envs', 4)
    eval_n_repeats = config['training'].get('eval_n_repeats', 2)
    
    # Ustawienia normalizacji z configu
    norm_config = config['training'].get('normalization', {})
    norm_obs = norm_config.get('norm_obs', False)
    norm_reward = norm_config.get('norm_reward', True)
    clip_obs = norm_config.get('clip_obs', 10.0)
    clip_reward = norm_config.get('clip_reward', 10.0)
    norm_gamma = norm_config.get('gamma', 0.99)
    epsilon = norm_config.get('epsilon', 1e-8)

    model_path_absolute = os.path.normpath(os.path.join(base_dir, config['paths']['model_path']))
    vec_norm_path = model_path_absolute.replace('.zip', '_vecnorm.pkl')
    vec_norm_eval_path = model_path_absolute.replace('.zip', '_vecnorm_eval.pkl')
    state_path = model_path_absolute.replace('.zip', '_state.pkl')

    load_model = os.path.exists(model_path_absolute)
    total_timesteps = 0
    
    if load_model:
        try:
            with open(state_path, 'rb') as f:
                state = pickle.load(f)
                total_timesteps = state['total_timesteps']
            print(f"Wznowienie treningu od {total_timesteps} krokÃ³w.")
        except Exception as e:
            print(f"BÅ‚Ä…d Å‚adowania stanu: {e}. Zaczynam od zera.")
        
        try:
            resp = input(f"Znaleziono istniejÄ…cy model pod {model_path_absolute}. Czy kontynuowaÄ‡ trening? [[Y]/n]: ").strip()
        except Exception:
            resp = ''

        if resp.lower() in ('n', 'no'):
            print("UÅ¼ytkownik wybraÅ‚ rozpoczÄ™cie treningu od nowa. Zaczynam od zera i uÅ¼ywam hyperparametrÃ³w z configu.")
            load_model = False
            total_timesteps = 0
            use_config_hyperparams = True
            
            # âœ… RESETOWANIE CSV: train_progress.csv
            try:
                train_csv = os.path.join(base_dir, config['paths']['train_csv_path'])
                if os.path.exists(train_csv):
                    os.remove(train_csv)
                    print(f"UsuniÄ™to istniejÄ…cy plik postÄ™pu treningu: {train_csv}")
            except Exception as e:
                print(f"Nie udaÅ‚o siÄ™ usunÄ…Ä‡ pliku postÄ™pu treningu: {e}")
            
            # âœ… RESETOWANIE CSV: gradient_monitor.csv
            try:
                gradient_csv = os.path.join(base_dir, 'logs', 'gradient_monitor.csv')
                if os.path.exists(gradient_csv):
                    os.remove(gradient_csv)
                    print(f"UsuniÄ™to istniejÄ…cy plik gradient monitor: {gradient_csv}")
            except Exception as e:
                print(f"Nie udaÅ‚o siÄ™ usunÄ…Ä‡ pliku gradient monitor: {e}")
            
            # âœ… RESETOWANIE LOG: victories.log
            try:
                victory_log = os.path.join(base_dir, 'logs', 'victories.log')
                if os.path.exists(victory_log):
                    os.remove(victory_log)
                    print(f"UsuniÄ™to istniejÄ…cy plik victory log: {victory_log}")
            except Exception as e:
                print(f"Nie udaÅ‚o siÄ™ usunÄ…Ä‡ pliku victory log: {e}")
        else:
            try:
                resp2 = input("UÅ¼yÄ‡ hyperparametrÃ³w z configu zamiast z modelu? [[Y]/n]: ").strip()
            except Exception:
                resp2 = ''
            use_config_hyperparams = False if resp2.lower() in ('n', 'no') else True

    reset_channel_logs(base_dir)
    if enable_channel_logs:
        channel_loggers.update(init_channel_loggers(base_dir))

    # WyÅ›wietl info o normalizacji
    print(f"\n{'='*70}")
    print(f"[NORMALIZATION CONFIG]")
    print(f"{'='*70}")
    print(f"  norm_obs:        {norm_obs} {'âŒ DISABLED (recommended for images)' if not norm_obs else 'âœ… ENABLED'}")
    print(f"  norm_reward:     {norm_reward} {'âœ… ENABLED (stabilizes training)' if norm_reward else 'âŒ DISABLED'}")
    print(f"  clip_obs:        {clip_obs}")
    print(f"  clip_reward:     {clip_reward}")
    print(f"  gamma:           {norm_gamma}")
    print(f"  epsilon:         {epsilon}")
    print(f"{'='*70}\n")

    # Tworzenie Å›rodowisk treningowych
    env = make_vec_env(make_env(render_mode=None, grid_size=None), n_envs=n_envs, vec_env_cls=SubprocVecEnv)
    if load_model and os.path.exists(vec_norm_path):
        env = VecNormalize.load(vec_norm_path, env)
        print(f"âœ… ZaÅ‚adowano VecNormalize z {vec_norm_path}")
    else:
        env = VecNormalize(
            env, 
            norm_obs=norm_obs,
            norm_reward=norm_reward,
            clip_obs=clip_obs,
            clip_reward=clip_reward,
            gamma=norm_gamma,
            epsilon=epsilon
        )
        print(f"âœ… Utworzono nowy VecNormalize z ustawieniami z config.yaml")

    # Tworzenie Å›rodowisk ewaluacyjnych
    eval_env = make_vec_env(make_env(render_mode=None, grid_size=16), n_envs=eval_n_envs, vec_env_cls=SubprocVecEnv)
    if load_model and os.path.exists(vec_norm_eval_path):
        eval_env = VecNormalize.load(vec_norm_eval_path, eval_env)
        print(f"âœ… ZaÅ‚adowano eval VecNormalize z {vec_norm_eval_path}")
    else:
        eval_env = VecNormalize(
            eval_env,
            norm_obs=norm_obs,
            norm_reward=norm_reward,
            clip_obs=clip_obs,
            clip_reward=clip_reward,
            gamma=norm_gamma,
            epsilon=epsilon
        )
        print(f"âœ… Utworzono nowy eval VecNormalize z ustawieniami z config.yaml")

    # ğŸ§¹ WyczyÅ›Ä‡ pamiÄ™Ä‡ przed utworzeniem modelu
    clear_gpu_cache()

    # Tworzenie lub Å‚adowanie modelu
    policy_kwargs = config['model']['policy_kwargs'].copy()
    policy_kwargs['features_extractor_class'] = CustomFeaturesExtractor

    if load_model:
        model = RecurrentPPO.load(model_path_absolute, env=env)
        model.ent_coef = config['model']['ent_coef']
        model.learning_rate = linear_schedule(config['model']['learning_rate'], config['model']['min_learning_rate'])
        model._setup_lr_schedule()
        
        # âœ… Skonfiguruj AdamW dla zaÅ‚adowanego modelu
        model = setup_adamw_optimizer(model, config)
        
        # ğŸ§¹ WyczyÅ›Ä‡ cache po zaÅ‚adowaniu
        clear_gpu_cache()
    else:
        model = RecurrentPPO(
            config['model']['policy'],
            env,
            learning_rate=linear_schedule(config['model']['learning_rate'], config['model']['min_learning_rate']),
            n_steps=config['model']['n_steps'],
            batch_size=config['training']['batch_size'],
            n_epochs=config['model']['n_epochs'],
            gamma=config['model']['gamma'],
            gae_lambda=config['model']['gae_lambda'],
            clip_range=config['model']['clip_range'],
            ent_coef=config['model']['ent_coef'],
            vf_coef=config['model']['vf_coef'],
            policy_kwargs=policy_kwargs,
            verbose=1,
            device=config['model']['device']
        )
        
        # âœ… Skonfiguruj AdamW dla nowego modelu
        model = setup_adamw_optimizer(model, config)

    if use_config_hyperparams and load_model:
        policy_kwargs = config['model']['policy_kwargs'].copy()
        policy_kwargs['features_extractor_class'] = CustomFeaturesExtractor
        model_new = RecurrentPPO(
            config['model']['policy'],
            env,
            learning_rate=linear_schedule(config['model']['learning_rate'], config['model']['min_learning_rate']),
            n_steps=config['model']['n_steps'],
            batch_size=config['training']['batch_size'],
            n_epochs=config['model']['n_epochs'],
            gamma=config['model']['gamma'],
            gae_lambda=config['model']['gae_lambda'],
            clip_range=config['model']['clip_range'],
            ent_coef=config['model']['ent_coef'],
            vf_coef=config['model']['vf_coef'],
            policy_kwargs=policy_kwargs,
            verbose=1,
            device=config['model']['device']
        )
        model_tmp = RecurrentPPO.load(model_path_absolute)
        model_new.policy.load_state_dict(model_tmp.policy.state_dict())
        
        # ğŸ§¹ UsuÅ„ stary model
        del model
        model = model_new
        del model_tmp
        
        # âœ… Skonfiguruj AdamW po przeniesieniu wag
        model = setup_adamw_optimizer(model, config)

    # Zastosuj gradient clipping
    apply_gradient_clipping(model, clip_value=2.0)

    global best_model_save_path
    best_model_save_path = os.path.normpath(os.path.join(base_dir, config['paths']['models_dir']))

    # Callbacki
    stop_on_plateau = StopTrainingOnNoModelImprovement(
        max_no_improvement_evals=config['training']['max_no_improvement_evals'],
        min_evals=config['training']['min_evals'],
        verbose=1
    )

    plot_script_path = os.path.join(os.path.dirname(__file__), 'utils', 'plot_train_progress.py')
    n_eval_episodes = eval_n_envs * eval_n_repeats
    
    eval_callback = CustomEvalCallback(
        eval_env=eval_env,
        best_model_save_path=best_model_save_path,
        log_path=os.path.normpath(os.path.join(base_dir, config['paths']['logs_dir'])),
        eval_freq=eval_freq,
        deterministic=True,
        render=False,
        callback_after_eval=stop_on_plateau,
        plot_interval=plot_interval,
        plot_script_path=plot_script_path,
        initial_timesteps=total_timesteps,
        n_eval_episodes=n_eval_episodes
    )

    train_progress_callback = TrainProgressCallback(
        os.path.join(base_dir, config['paths']['train_csv_path']),
        initial_timesteps=total_timesteps
    )
    
    loss_recorder = LossRecorderCallback()
    
    # Entropy scheduler - zmniejsza entropiÄ™ liniowo w czasie
    entropy_scheduler = EntropySchedulerCallback(
        entropy_schedule_fn=entropy_schedule(
            config['model']['ent_coef'], 
            config['model'].get('min_ent_coef', 0.001)
        ),
        initial_timesteps=total_timesteps,
        verbose=1
    )
    
    print(f"\n{'='*70}")
    print(f"[ENTROPY SCHEDULE]")
    print(f"{'='*70}")
    print(f"  Initial ent_coef:  {config['model']['ent_coef']}")
    print(f"  Min ent_coef:      {config['model'].get('min_ent_coef', 0.001)}")
    print(f"  Schedule:          Linear decay (similar to learning rate)")
    print(f"{'='*70}\n")

    # âœ… Gradient & Weight Monitor
    gradient_monitor = GradientWeightMonitor(
        csv_path=os.path.join(base_dir, 'logs', 'gradient_monitor.csv'),
        log_freq=config['training'].get('gradient_log_freq', 2000)
    )
    
    # âœ… Victory Tracker
    victory_tracker = VictoryTrackerCallback(
        log_dir=os.path.join(base_dir, 'logs'),
        verbose=1
    )

    # Debug logging (jeÅ›li wÅ‚Ä…czone)
    if enable_channel_logs:
        for logger in channel_loggers.values():
            logger.info(f"\n--- Debug: RozpoczÄ™cie treningu ---")
        debug_env = make_env(render_mode=None, grid_size=None)()
        obs, _ = debug_env.reset()
        grid_size = debug_env.grid_size
        log_observation(obs, channel_loggers, grid_size, step=0)
        for debug_step in range(5):
            action = debug_env.action_space.sample()
            obs, reward, terminated, truncated, info = debug_env.step(action)
            log_observation(obs, channel_loggers, grid_size, step=debug_step + 1)
            if enable_channel_logs:
                for logger in channel_loggers.values():
                    logger.info(f"Akcja: {action}, Nagroda: {reward}, Terminated: {terminated}, Truncated: {truncated}, Info: {info}")
            if terminated or truncated:
                break
        debug_env.close()
        if enable_channel_logs:
            for logger in channel_loggers.values():
                logger.info(f"--- Koniec debug dla grid_size={grid_size} ---")

    # Rozpocznij trening
    try:
        configured_total = config['training'].get('total_timesteps', 0)
        remaining_timesteps = configured_total - total_timesteps
        
        try:
            if configured_total > 0 and total_timesteps / configured_total >= 0.8:
                print(f"UÅ¼yto {total_timesteps}/{configured_total} krokÃ³w ({total_timesteps/configured_total:.1%}). To >=80% limitu.")
                extra = input("Ile dodatkowych krokÃ³w dodaÄ‡? (0 = brak, domyÅ›lnie 0): ").strip()
                try:
                    extra_int = int(extra) if extra != '' else 0
                except ValueError:
                    print("Niepoprawna wartoÅ›Ä‡, uÅ¼ywam 0 dodatkowych krokÃ³w.")
                    extra_int = 0
                remaining_timesteps += extra_int
        except Exception:
            pass
        
        if remaining_timesteps > 0:
            model.learn(
                total_timesteps=remaining_timesteps,
                reset_num_timesteps=not load_model,
                callback=[
                    eval_callback, 
                    train_progress_callback, 
                    loss_recorder, 
                    entropy_scheduler, 
                    gradient_monitor,
                    victory_tracker
                ],
                progress_bar=use_progress_bar,
                tb_log_name=f"recurrent_ppo_snake_{total_timesteps}"
            )
        else:
            print(f"Trening zakoÅ„czony: osiÄ…gniÄ™to {total_timesteps} krokÃ³w.")
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Przerwanie treningu przez uÅ¼ytkownika (Ctrl+C)")
        print("Zamykanie Å›rodowisk...")
        try:
            env.close()
            eval_env.close()
        except:
            pass
        # ğŸ§¹ WyczyÅ›Ä‡ pamiÄ™Ä‡
        clear_gpu_cache()
        raise
    except Exception as e:
        print(f"BÅ‚Ä…d podczas treningu: {e}")
        raise
    finally:
        try:
            env.close()
            eval_env.close()
        except:
            pass
        # ğŸ§¹ WyczyÅ›Ä‡ pamiÄ™Ä‡ na koniec
        clear_gpu_cache()

    print("Trening zakoÅ„czony!")


if __name__ == "__main__":
    try:
        train(use_progress_bar=True)
    except KeyboardInterrupt:
        print("\n\nTrening przerwany.")
        # ğŸ§¹ Finalne czyszczenie
        clear_gpu_cache()
        os._exit(0)
    except Exception as e:
        print(f"BÅ‚Ä…d podczas treningu: {e}")
        import traceback
        traceback.print_exc()
        # ğŸ§¹ Finalne czyszczenie
        clear_gpu_cache()
        sys.exit(1)
    print("Trening zakoÅ„czony!")