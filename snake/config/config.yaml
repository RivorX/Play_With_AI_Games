# Konfiguracja środowiska Snake
environment:
  min_grid_size: 5                   # Minimalny rozmiar siatki
  max_grid_size: 25                  # Maksymalny rozmiar siatki
  viewport_size: 16                  # Rozmiar viewport (stały dla wszystkich grid_size)
  snake_size: 20                     # Rozmiar kratki w pikselach
  directions: [[0, -1], [1, 0], [0, 1], [-1, 0]]  # Kierunki ruchu węża
  observation_space:
    low: -1.0                        # Minimalna wartość obserwacji
    high: 1.0                        # Maksymalna wartość obserwacji
    dtype: float32                   # Typ danych obserwacji
  action_space:
    type: Discrete                   # Typ przestrzeni akcji
    n: 3                             # Liczba akcji (lewo, prosto, prawo)
  max_steps_without_food: 20         # Max kroków bez jedzenia (skalowane z rozmiarem)
  max_steps_factor: 200              # Mnożnik max kroków

# Konfiguracja modelu RecurrentPPO
model:
  policy: MultiInputLstmPolicy       # Policy dla RecurrentPPO
  learning_rate: 0.0003              # Szybkość uczenia 
  min_learning_rate: 0.00005         # Minimalny learning rate
  n_steps: 512                       # Liczba kroków na aktualizację
  n_epochs: 3                        # Liczba epok 
  gamma: 0.99                        # Współczynnik dyskontowania
  gae_lambda: 0.95                   # Lambda dla GAE
  clip_range: 0.2                    # Zakres obcinania PPO
  ent_coef: 0.05                     # Współczynnik entropii (początkowy)
  min_ent_coef: 0.01                 # Minimalny współczynnik entropii (docelowy)
  vf_coef: 0.5                       # Współczynnik funkcji wartości
  device: cuda                       # Urządzenie (cuda/cpu)
  
  # ✅ NOWE: Konfiguracja optymalizatora
  optimizer:
    type: adamw                      # Typ: 'adam' lub 'adamw'
    weight_decay: 0.001               # Weight decay dla AdamW (0.0 = wyłączone)
    eps: 1.0e-6                      # Epsilon dla stabilności
    betas: [0.9, 0.999]              # Beta1, Beta2 dla Adam/AdamW
  
  # Selektywne dropouty - WYMUSZAJĄ używanie CNN!
  cnn_dropout: 0.0                   # Dropout dla CNN
  scalar_dropout: 0.01               # Dropout dla skalarów
  scalar_input_dropout: 0.0          # Dropout dla wejścia skalarów
  fusion_dropout: 0.01               # Dropout po fuzji
  convlstm:
    cnn_channels: [32, 64]           # Liczba kanałów w warstwach konwolucyjnych
    scalar_hidden_dims: [64]         # Ukryte wymiary dla danych skalarnych 

  policy_kwargs:
    features_extractor_kwargs:
      features_dim: 512              # Wymiar cech wyjściowych ekstraktora
    net_arch:
      pi: [256, 128]                 # Wielkość sieci aktora
      vf: [256, 128]                 # Wielkość sieci krytyka
    lstm_hidden_size: 256            # Rozmiar ukryty LSTM
    n_lstm_layers: 1                 # Liczba warstw LSTM
    enable_critic_lstm: false         # LSTM także dla krytyka

# Konfiguracja treningu
training:
  n_envs: 12                         # Liczba środowisk
  batch_size: 512                    # Rozmiar partii
  total_timesteps: 10000000          # Całkowita liczba kroków
  eval_freq: 4000                    # Częstotliwość ewaluacji
  plot_interval: 1                   # Co ile walidacji robić wykres i zapisywać model
  max_no_improvement_evals: 30       # Liczba ewaluacji bez poprawy przed zatrzymaniem
  min_evals: 5                       # Minimalna liczba ewaluacji przed sprawdzeniem zatrzymania
  enable_channel_logs: false         # Włącz logi kanałów
  gradient_log_freq: 2000            # ✅ NOWE: Co ile kroków logować gradienty/wagi
  # Liczba środowisk do ewaluacji równoległej
  eval_n_envs: 6                    # Liczba środowisk ewaluacyjnych
  eval_n_repeats: 2                 # Ile razy każde środowisko ma wykonać ewaluację
  
  # Konfiguracja normalizacji (PRZEŁĄCZNIKI)
  normalization:
    norm_obs: false                  # ❌ FALSE dla obrazów (CNN radzi sobie z surowymi pikselami)
    norm_reward: true                # ✅ TRUE - stabilizuje trening (running mean rewards)
    clip_obs: 10.0                   # Clipping obserwacji (tylko jeśli norm_obs=true)
    clip_reward: 10.0                # Clipping nagród (10.0 = standardowa wartość)
    gamma: 0.99                      # Gamma dla running mean rewards (powinno = model.gamma)
    
    # DODATKOWE OPCJE (zaawansowane)
    norm_obs_keys: []                # Lista kluczy do normalizacji (puste = wszystkie, jeśli norm_obs=true)
                                     # Przykład: ['dx_head', 'dy_head'] - tylko skalary
    epsilon: 1.0e-8                  # Epsilon dla stabilności numerycznej
    clip_obs_asymmetric: false       # Asymetryczne clipping: (-clip_obs, clip_obs) vs (0, clip_obs)

# Ścieżki do zapisu
paths:
  models_dir: models
  logs_dir: logs
  model_path: models/snake_ppo_model.zip
  train_csv_path: logs/train_progress.csv
  plot_path: logs/training_progress.png
  test_progress_path: logs/test_progress.png