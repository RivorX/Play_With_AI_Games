# Konfiguracja ≈örodowiska Snake
environment:
  min_grid_size: 8                   # Minimalny rozmiar siatki
  max_grid_size: 25                  # Maksymalny rozmiar siatki
  viewport_size: 16                  # Rozmiar viewport (sta≈Çy dla wszystkich grid_size)
  snake_size: 20                     # Rozmiar kratki w pikselach
  directions: [[0, -1], [1, 0], [0, 1], [-1, 0]]  # Kierunki ruchu wƒô≈ºa
  observation_space:
    low: -1.0                        # Minimalna warto≈õƒá obserwacji
    high: 1.0                        # Maksymalna warto≈õƒá obserwacji
    dtype: float32                   # Typ danych obserwacji
  action_space:
    type: Discrete                   # Typ przestrzeni akcji
    n: 3                             # Liczba akcji (lewo, prosto, prawo)
  max_steps_without_food: 20         # Max krok√≥w bez jedzenia (skalowane z rozmiarem grid_size * max_steps_without_food)
  max_steps_factor: 200              # Mno≈ºnik max krok√≥w (skalowany z rozmiarem grid_size * max_steps_factor)

  reward_scaling:
    enable: true                          # W≈ÇƒÖcz skalowanie nagr√≥d
    min_difficulty_multiplier: 1.0        # Mno≈ºnik dla min_grid_size
    max_difficulty_multiplier: 2.5        # Mno≈ºnik dla max_grid_size
    
    # Nagrody bazowe
    base_food_reward: 10.0                # Podstawowa nagroda za jab≈Çko
    base_death_penalty: -10.0             # Podstawowa kara za ≈õmierƒá
    step_penalty: -0.03                   # ‚¨ÜÔ∏è ZWIƒòKSZONE z -0.01 (zachƒôca do efektywno≈õci)
    
    # ‚¨áÔ∏è Milestone bonusy (% zajƒôcia planszy)
    milestones:
      0.10: 5.0       # ‚¨áÔ∏è Z 10.0
      0.20: 15.0      # ‚¨áÔ∏è Z 30.0
      0.30: 35.0      # ‚¨áÔ∏è Z 70.0
      0.40: 70.0      # ‚¨áÔ∏è Z 140.0
      0.50: 150.0     # ‚¨áÔ∏è Z 280.0
      0.60: 300.0     # ‚¨áÔ∏è Z 600.0
      0.70: 600.0     # ‚¨áÔ∏è Z 1200.0
      0.80: 1200.0    # ‚¨áÔ∏è Z 2500.0
      0.90: 2500.0    # ‚¨áÔ∏è Z 5000.0
      1.00: 10000.0   # üèÜ Epic win!

    # Opcjonalne: bonus za efektywno≈õƒá (ma≈Çe steps_per_apple)
    efficiency_bonus:
      enable: true
      threshold: 18.0               # ‚¨ÜÔ∏è z 15.0 (≈Çatwiej osiƒÖgnƒÖƒá)
      reward: 3.0                   # ‚¨ÜÔ∏è z 2.0 (wiƒôksza zachƒôta)

# Konfiguracja modelu RecurrentPPO
model:
  policy: MultiInputLstmPolicy       # Policy dla RecurrentPPO
  learning_rate: 0.0002              # Szybko≈õƒá uczenia 
  min_learning_rate: 0.00001         # Minimalny learning rate
  n_steps: 1024                      # Liczba krok√≥w na aktualizacjƒô
  n_epochs: 4                        # Liczba epok 
  gamma: 0.99                        # Wsp√≥≈Çczynnik dyskontowania
  gae_lambda: 0.95                   # Lambda dla GAE
  clip_range: 0.2                    # Zakres obcinania PPO
  ent_coef: 0.05                     # Wsp√≥≈Çczynnik entropii (poczƒÖtkowy)
  min_ent_coef: 0.03                 # Minimalny wsp√≥≈Çczynnik entropii (docelowy)
  vf_coef: 0.75                      # Wsp√≥≈Çczynnik funkcji warto≈õci
  device: cuda                       # UrzƒÖdzenie (cuda/cpu)
  
  # ‚úÖ Konfiguracja optymalizatora
  optimizer:
    type: adamw                      # Typ: 'adam' lub 'adamw'
    weight_decay: 0.005              # Weight decay dla AdamW (0.0 = wy≈ÇƒÖczone)
    eps: 1.0e-7                      # Epsilon dla stabilno≈õci
    betas: [0.9, 0.999]              # Beta1, Beta2 dla Adam/AdamW

  # üÜï LSTM Configuration
  lstm:
    gradient_clip_val: 2.0           # Clip gradients LSTM
    cell_state_penalty: 0.001        # L2 penalty na cell state
    hidden_state_penalty: 0.0001     # L2 penalty na hidden state
    dropout: 0.01                     # Dropout w LSTM
  
  # Selektywne dropouty
  cnn_dropout: 0.0                 
  scalar_dropout: 0.01               # Dropout dla skalar√≥w
  scalar_input_dropout: 0.0          # Dropout dla wej≈õcia skalar√≥w
  fusion_dropout: 0.01              
  
  # ‚úÖ DEEP BOTTLENECK CNN ARCHITECTURE
  convlstm:
    cnn_channels: [32, 64]           # Kana≈Çy CNN (bez zmian)
    
    # Deep Bottleneck Layers
    # Main path: 4096 ‚Üí 1024 ‚Üí 640
    cnn_bottleneck_dims: [2048, 1024]
    cnn_output_dim: 768
    
    # üÜï Scalar Network Architecture  
    scalar_hidden_dims: [128, 192] 
    
    # üÜï Architecture Options
    use_layernorm: true              # LayerNorm
    learnable_alpha: true            # Learnable alpha
    initial_alpha: 0.5               # PoczƒÖtkowa waga residual

  policy_kwargs:
    features_extractor_kwargs:
      features_dim: 768              # Wymiar cech wyj≈õciowych ekstraktora
    net_arch:
      pi: [512, 256]                 # ‚¨ÜÔ∏è OPTYMALNE: bez bottlenecku po LSTM
      vf: [512, 256]                 # ‚¨ÜÔ∏è OPTYMALNE: lepsza wycena warto≈õci
    lstm_hidden_size: 512            # Rozmiar ukryty LSTM
    n_lstm_layers: 1                 # Liczba warstw LSTM
    enable_critic_lstm: false        # LSTM dla krytyka

# Konfiguracja treningu
training:
  n_envs: 6                          # Liczba ≈õrodowisk
  batch_size: 1024                   # Rozmiar partii
  total_timesteps: 2000000           # Ca≈Çkowita liczba krok√≥w
  eval_freq: 4000                    # Czƒôstotliwo≈õƒá ewaluacji
  plot_interval: 1                   # Co ile walidacji robiƒá wykres i zapisywaƒá model
  max_no_improvement_evals: 30       # Liczba ewaluacji bez poprawy przed zatrzymaniem
  min_evals: 10                      # Minimalna liczba ewaluacji przed sprawdzeniem zatrzymania
  enable_channel_logs: false         # W≈ÇƒÖcz logi kana≈Ç√≥w
  gradient_log_freq: 2000            # ‚úÖ Co ile krok√≥w logowaƒá gradienty/wagi
  # Liczba ≈õrodowisk do ewaluacji r√≥wnoleg≈Çej
  eval_n_envs: 6                     # Liczba ≈õrodowisk ewaluacyjnych
  eval_n_repeats: 2                  # Ile razy ka≈ºde ≈õrodowisko ma wykonaƒá ewaluacjƒô
  
  # Konfiguracja normalizacji
  normalization:
    norm_obs: false                  # ‚ùå FALSE dla obraz√≥w (CNN radzi sobie z surowymi pikselami)
    norm_reward: true                # ‚úÖ TRUE - stabilizuje trening (running mean rewards)
    clip_obs: 10.0                   # Clipping obserwacji (tylko je≈õli norm_obs=true)
    clip_reward: 10.0                # Clipping nagr√≥d (10.0 = standardowa warto≈õƒá)
    gamma: 0.99                      # Gamma dla running mean rewards (powinno = model.gamma)
    
    # DODATKOWE OPCJE (zaawansowane)
    norm_obs_keys: []                # Lista kluczy do normalizacji (puste = wszystkie, je≈õli norm_obs=true)
    epsilon: 1.0e-8                  # Epsilon dla stabilno≈õci numerycznej
    clip_obs_asymmetric: false       # Asymetryczne clipping

# ≈öcie≈ºki do zapisu
paths:
  models_dir: models
  logs_dir: logs
  model_path: models/snake_ppo_model.zip
  train_csv_path: logs/train_progress.csv
  plot_path: logs/training_progress.png
  test_progress_path: logs/test_progress.png