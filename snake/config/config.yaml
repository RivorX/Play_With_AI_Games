# Konfiguracja środowiska Snake
environment:
  grid_size: 15
  snake_size: 20
  directions: [[0, -1], [1, 0], [0, 1], [-1, 0]]
  observation_space: { low: 0, high: 3, shape: [37], dtype: float32 }  # Zaktualizowano shape (6 kolizji + 4 kierunki + 2 odległości + 25 mini-mapa)
  action_space: { type: Discrete, n: 3 }
  max_steps_without_food: 100  # Zwiększono limit kroków bez jedzenia, aby zmniejszyć zamykanie się
  max_steps_factor: 150  # Zwiększono faktor maksymalnych kroków, aby dać więcej czasu na manewry

# Konfiguracja modelu PPO
model:
  policy: MlpPolicy
  learning_rate: 0.0003
  n_steps: 8192
  batch_size: 4096
  n_epochs: 10
  gamma: 0.999
  gae_lambda: 0.98
  clip_range: 0.15
  ent_coef: 0.03
  vf_coef: 0.5
  device: cuda
  policy_kwargs:
    net_arch: [512, 512, 256]  # Zwiększono architekturę sieci, aby obsłużyć większą przestrzeń obserwacji

# Konfiguracja treningu
training:
  n_envs: 32
  total_timesteps: 100000000
  eval_freq: 10000
  max_no_improvement_evals: 8
  min_evals: 5
  enable_testing: false  # Przełącznik do włączania/wyłączania testowania podczas treningu

# Ścieżki do zapisu
paths:
  models_dir: models
  logs_dir: logs
  model_path: models/snake_ppo_model.zip
  test_model_path: models/test_model.zip
  best_model_path: models/best_model.zip
  train_csv_path: logs/train_progress.csv
  test_csv_path: logs/test_results.csv
  test_log_path: logs/test_log.txt
  plot_path: logs/training_progress.png
  test_progress_path: logs/test_progress.png