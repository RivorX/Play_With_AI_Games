# Konfiguracja ≈örodowiska Snake
environment:
  min_grid_size: 8                   # Minimalny rozmiar siatki
  max_grid_size: 25                  # Maksymalny rozmiar siatki
  viewport_size: 12                  # Rozmiar viewport (sta≈Çy dla wszystkich grid_size)
  snake_size: 20                     # Rozmiar kratki w pikselach
  directions: [[0, -1], [1, 0], [0, 1], [-1, 0]]  # Kierunki ruchu wƒô≈ºa
  observation_space:
    low: -1.0                        # Minimalna warto≈õƒá obserwacji
    high: 1.0                        # Maksymalna warto≈õƒá obserwacji
    dtype: float32                   # Typ danych obserwacji
  action_space:
    type: Discrete                   # Typ przestrzeni akcji
    n: 3                             # Liczba akcji (lewo, prosto, prawo)
  max_steps_without_food: 30         # Max krok√≥w bez jedzenia (skalowane z rozmiarem grid_size * max_steps_without_food)
  max_steps_factor: 300              # Mno≈ºnik max krok√≥w (skalowany z rozmiarem grid_size * max_steps_factor)

  reward_scaling:
    enable: true                          # W≈ÇƒÖcz skalowanie nagr√≥d
    min_difficulty_multiplier: 1.0        # Mno≈ºnik dla min_grid_size
    max_difficulty_multiplier: 3.0        # Mno≈ºnik dla max_grid_size
    
    # Nagrody bazowe
    base_food_reward: 10.0                # Podstawowa nagroda za jab≈Çko
    base_death_penalty: -5.0              # Podstawowa kara za ≈õmierƒá
    
    # Milestone bonusy (% zajƒôcia planszy)
    milestones:
      0.10: 5.0   
      0.20: 15.0  
      0.30: 35.0   
      0.40: 70.0  
      0.50: 150.0    
      0.60: 300.0   
      0.70: 500.0  
      0.80: 700.0  
      0.90: 1000.0   
      1.00: 1500.0  

    # Opcjonalne: bonus za efektywno≈õƒá (ma≈Çe steps_per_apple)
    efficiency_bonus:
      enable: false                # W≈ÇƒÖcz/wy≈ÇƒÖcz
      threshold: 18.0               # ‚¨ÜÔ∏è z 15.0 (≈Çatwiej osiƒÖgnƒÖƒá)
      reward: 3.0                   # ‚¨ÜÔ∏è z 2.0 (wiƒôksza zachƒôta)

    # ‚ú® PROGRESSIVE BONUS - +3% za ka≈ºde kolejne jab≈Çko
    progressive_food_bonus:
      enable: true
      bonus_per_apple: 0.03           # +3% za ka≈ºde jab≈Çko
      max_multiplier: 3.0              # Cap na 300%

# Konfiguracja modelu RecurrentPPO
model:
  policy: MultiInputLstmPolicy       # Policy dla RecurrentPPO
  learning_rate: 0.0002              # Szybko≈õƒá uczenia 
  min_learning_rate: 0.00001         # Minimalny learning rate
  n_steps: 1024                      # ‚úÖ Liczba krok√≥w na aktualizacjƒô (zwiƒôkszone z 512)
  n_epochs: 3                        # ‚úÖ Liczba epok (zmniejszone z 4)
  gamma: 0.99                        # Wsp√≥≈Çczynnik dyskontowania
  gae_lambda: 0.95                   # Lambda dla GAE
  clip_range: 0.2                    # Zakres obcinania PPO
  ent_coef: 0.05                     # Wsp√≥≈Çczynnik entropii (poczƒÖtkowy)
  min_ent_coef: 0.01                 # Minimalny wsp√≥≈Çczynnik entropii (docelowy)
  vf_coef: 0.8                       # Wsp√≥≈Çczynnik funkcji warto≈õci
  device: cuda                       # UrzƒÖdzenie (cuda/cpu)

  # ‚úÖ FIXED: Balanced gradient scaling (was too extreme)
  cnn_gradient_scale: 5.0            # ‚¨ÜÔ∏è z 3.0 (by≈Ço za s≈Çabe)
  scalar_gradient_scale: 1.0         # ‚¨ÜÔ∏è z 0.8 (scalars muszƒÖ siƒô uczyƒá!)
  
  # ‚úÖ Konfiguracja optymalizatora
  optimizer:
    type: adamw                      # Typ: 'adam' lub 'adamw'
    weight_decay: 0.001              # Weight decay dla AdamW (0.0 = wy≈ÇƒÖczone)
    eps: 1.0e-8                      # Epsilon dla stabilno≈õci
    betas: [0.9, 0.999]              # Beta1, Beta2 dla Adam/AdamW

  # LSTM Configuration
  lstm:
    gradient_clip_val: 5.0          # Clip gradients LSTM
    cell_state_penalty: 0.0         # L2 penalty na cell state
    hidden_state_penalty: 0.0       # L2 penalty na hidden state
    dropout: 0.01                   # Dropout w LSTM
  
  # Selektywne dropouty
  cnn_dropout: 0.05                 
  scalar_dropout: 0.05                # Dropout dla skalar√≥w
  scalar_input_dropout: 0.0          # Dropout dla wej≈õcia skalar√≥w
  fusion_dropout: 0.01              
  
  # CNN ARCHITECTURE
  convlstm:
    cnn_channels: [32, 64, 128]           # Kana≈Çy CNN
    
    # Bottleneck: Single-layer (najszybszy)
    cnn_bottleneck_dims: []              # Empty = 4608 ‚Üí 640 w jednym kroku
    cnn_output_dim: 640
    
    # Scalar Network Architecture  
    scalar_hidden_dims: [128, 128] 
    
    # Architecture Options
    use_layernorm: true          # Global switch
    
    # üéØ Fine-grained LayerNorm control:
    layernorm:
      cnn_bottleneck: true       # ‚úÖ ON - stabilizuje bottleneck output
      scalars: true              # ‚úÖ ON - stabilizuje ma≈Çe warto≈õci
      fusion: true               # ‚úÖ ON - stabilizuje LSTM input

    # üÜï ULTIMATE CNN Features
    spatial_attention: true            # ‚úÖ ENABLE - focus on important regions (+3-5%)
    stochastic_depth_prob: 0.1         # 10% drop path probability (regularization)
    residual_scale: 0.2                # Scale residual by 0.2x (prevent domination)

  policy_kwargs:
    features_extractor_kwargs:
      features_dim: 512              # Wymiar cech wyj≈õciowych ekstraktora
    net_arch:
      pi: [512, 256]               
      vf: [512, 256]               
    lstm_hidden_size: 512            # Rozmiar ukryty LSTM
    n_lstm_layers: 1                 # Liczba warstw LSTM
    enable_critic_lstm: true         # LSTM dla krytyka

# Konfiguracja treningu
training:
  n_envs: 8                          # ‚úÖ Liczba ≈õrodowisk (zwiƒôkszone z 6)
  batch_size: 1024                   # ‚úÖ Rozmiar partii (zwiƒôkszone z 1024)
  total_timesteps: 3000000           # Ca≈Çkowita liczba krok√≥w
  eval_freq: 8192                    # ‚úÖ Czƒôstotliwo≈õƒá ewaluacji (zwiƒôkszone z 4000)
  plot_interval: 1                   # Co ile walidacji robiƒá wykres i zapisywaƒá model
  max_no_improvement_evals: 30       # Liczba ewaluacji bez poprawy przed zatrzymaniem
  min_evals: 10                      # Minimalna liczba ewaluacji przed sprawdzeniem zatrzymania
  enable_channel_logs: false         # W≈ÇƒÖcz logi kana≈Ç√≥w
  gradient_log_freq: 10000           # Co ile krok√≥w logowaƒá gradienty/wagi
  vecnorm_reset_freq: 50000          # Co ile krok√≥w resetowaƒá VecNormalize
  
  # Liczba ≈õrodowisk do ewaluacji r√≥wnoleg≈Çej
  eval_n_envs: 6                     # Liczba ≈õrodowisk ewaluacyjnych
  eval_n_repeats: 2                  # Ile razy ka≈ºde ≈õrodowisko ma wykonaƒá ewaluacjƒô
  
  # Konfiguracja normalizacji
  normalization:
    norm_obs: false                  # ‚ùå FALSE dla obraz√≥w (CNN radzi sobie z surowymi pikselami)
    norm_reward: true                # ‚úÖ TRUE - stabilizuje trening (running mean rewards)
    clip_obs: 10.0                   # Clipping obserwacji (tylko je≈õli norm_obs=true)
    clip_reward: 500.0               # Clipping nagr√≥d (zwiƒôkszone dla progressive bonus)
    gamma: 0.99                      # Gamma dla running mean rewards (powinno = model.gamma)
    
    # DODATKOWE OPCJE (zaawansowane)
    epsilon: 1.0e-8                  # Epsilon dla stabilno≈õci numerycznej
    clip_obs_asymmetric: false       # Asymetryczne clipping

# ≈öcie≈ºki
paths:
  models_dir: models
  logs_dir: logs
  model_path: models/snake_ppo_model.zip
  train_csv_path: logs/train_progress.csv
  plot_path: logs/training_progress.png
  test_progress_path: logs/test_progress.png