# Konfiguracja środowiska Snake
environment:
  min_grid_size: 5                   # Minimalny rozmiar siatki dla losowania
  max_grid_size: 25                  # Maksymalny rozmiar siatki (może być >16 dzięki viewport!)
  snake_size: 20                     # Rozmiar kratki w pikselach (dla renderowania)
  directions: [[0, -1], [1, 0], [0, 1], [-1, 0]]  # Kierunki ruchu węża [lewo, dół, prawo, góra]
  observation_space:
    low: -1.0                        # Minimalna wartość obserwacji (-1 = ściany)
    high: 1.0                        # Maksymalna wartość obserwacji
    dtype: float32                   # Typ danych obserwacji
  action_space:
    type: Discrete                    # Typ przestrzeni akcji
    n: 3                             # Liczba akcji (lewo, prosto, prawo)
  max_steps_without_food: 10         # Max kroków bez jedzenia (skalowane z rozmiarem)
  max_steps_factor: 150              # Mnożnik max kroków

# Konfiguracja modelu RecurrentPPO
model:
  policy: MultiInputLstmPolicy       # MultiInputLstmPolicy dla RecurrentPPO
  learning_rate: 0.0002              # Szybkość uczenia 
  min_learning_rate: 0.00005         # Minimalny learning rate (nie schodzi poniżej)
  n_steps: 1024                      # Liczba kroków na aktualizację
  n_epochs: 10                       # Liczba epok 
  gamma: 0.99                        # Współczynnik dyskontowania
  gae_lambda: 0.95                   # Lambda dla GAE
  clip_range: 0.2                    # Zakres obcinania PPO
  ent_coef: 0.2                      # Współczynnik entropii
  vf_coef: 1.0                       # Współczynnik funkcji wartości
  device: cuda                       # Urządzenie (cuda/cpu)
  dropout_rate: 0.2                  # Współczynnik dropout w CNN
  
  # Parametry CNN (ConvLSTM usunięty - RecurrentPPO ma własny LSTM)
  convlstm:
    cnn_channels: [32, 64]          # Kanały w warstwach CNN
    scalar_hidden_dims: [32]         # Rozmiary warstw ukrytych dla zmiennych skalarnych
  
  policy_kwargs:
    features_extractor_kwargs:
      features_dim: 512
    net_arch:
      pi: [256, 128]  # dla aktora
      vf: [256, 128]  # dla krytyka
    lstm_hidden_size: 128            # Rozmiar LSTM w RecurrentPPO
    n_lstm_layers: 1                 # Liczba warstw LSTM
    enable_critic_lstm: true         # LSTM także dla krytyka

# Konfiguracja treningu
training:
  n_envs: 8                          # Liczba środowisk
  batch_size: 512                    # Rozmiar partii
  total_timesteps: 10000000          # Całkowita liczba kroków
  eval_freq: 3000                    # Częstotliwość ewaluacji
  plot_interval: 1                   # Co ile walidacji robić wykres
  max_no_improvement_evals: 50       # Liczba ewaluacji bez poprawy przed zatrzymaniem
  min_evals: 5                       # Minimalna liczba ewaluacji
  enable_channel_logs: false         # Wyłącz logi kanałów (wydajność)

# Ścieżki do zapisu
paths:
  models_dir: models
  logs_dir: logs
  model_path: models/snake_ppo_model.zip
  train_csv_path: logs/train_progress.csv
  plot_path: logs/training_progress.png
  test_progress_path: logs/test_progress.png